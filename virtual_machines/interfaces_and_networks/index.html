<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><link rel="canonical" href="https://kubevirt.io/user-guide/virtual_machines/interfaces_and_networks/" />
      <link rel="shortcut icon" href="../../assets/favicon.ico" />
    <title>Interfaces and Networks - KubeVirt User-Guide</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Interfaces and Networks";
        var mkdocs_page_input_path = "virtual_machines/interfaces_and_networks.md";
        var mkdocs_page_url = "/user-guide/virtual_machines/interfaces_and_networks/";
      </script>
    
    <script src="../../js/jquery-3.6.0.min.js" defer></script>
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/languages/yaml.min.js"></script>
      <script>hljs.initHighlightingOnLoad();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../.." class="icon icon-home"> KubeVirt User-Guide
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../..">Welcome</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../architecture/">Architecture</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../release_notes/">KubeVirt release notes</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Operations</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../operations/installation/">Installation</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../operations/updating_and_deletion/">Updating and deletion</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../operations/basic_use/">Basic use</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../operations/api_validation/">API Validation</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../operations/debug/">Debug</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../operations/virtctl_client_tool/">virtctl Client Tool</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../operations/live_migration/">Live Migration</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../operations/hotplug_volumes/">Hotplug Volumes</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../operations/snapshot_restore_api/">Snapshot Restore API</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../operations/hugepages/">Hugepages support</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../operations/component_monitoring/">Component monitoring</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../operations/authorization/">Authorization</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../operations/annotations_and_labels/">Annotations and labels</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../operations/node_assignment/">Node assignment</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../operations/node_maintenance/">Node maintenance</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../operations/node_overcommit/">Node overcommit</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../operations/unresponsive_nodes/">Unresponsive nodes</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../operations/containerized_data_importer/">Containerized Data Importer</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../operations/activating_feature_gates/">Activating feature gates</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../operations/clone_api/">Clone API</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../operations/mediated_devices_configuration/">Mediated devices and virtual GPUs</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../operations/migration_policies/">Migration Policies</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Virtual machines</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="../virtual_machine_instances/">Virtual Machines Instances</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../lifecycle/">Lifecycle</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../run_strategies/">Run Strategies</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../instancetypes/">Instancetypes and preferences</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../presets/">Presets</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../virtual_hardware/">Virtual hardware</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../dedicated_cpu_resources/">Dedicated CPU resources</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../numa/">NUMA</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../disks_and_volumes/">Disks and Volumes</a>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" href="./">Interfaces and Networks</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#backend">Backend</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#frontend">Frontend</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#security">Security</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../istio_service_mesh/">Istio service mesh</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../networkpolicy/">NetworkPolicy</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../host-devices/">Host Devices Assignment</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../windows_virtio_drivers/">Windows virtio drivers</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../guest_operating_system_information/">Guest Operating System Information</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../guest_agent_information/">Guest Agent information</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../liveness_and_readiness_probes/">Liveness and Readiness Probes</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../accessing_virtual_machines/">Accessing Virtual Machines</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../startup_scripts/">Startup Scripts</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../service_objects/">Service objects</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../templates/">Templates</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tekton_tasks/">KubeVirt Tekton</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../replicaset/">VirtualMachineInstanceReplicaSet</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../dns/">DNS records</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../boot_from_external_source/">Booting From External Source</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../confidential_computing/">Confidential computing</a>
                  </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../web_console/">Web Console</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Appendix</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../appendix/contributing/">Contributing</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">KubeVirt User-Guide</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" alt="Docs"></a> &raquo;</li>
          <li>Virtual machines &raquo;</li>
      <li>Interfaces and Networks</li>
    <li class="wy-breadcrumbs-aside">
        <a href="https://github.com/kubevirt/user-guide/edit/main/docs/virtual_machines/interfaces_and_networks.md"
          class="icon icon-github"> Edit on GitHub</a>
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="interfaces-and-networks">Interfaces and Networks<a class="headerlink" href="#interfaces-and-networks" title="Permanent link">&para;</a></h1>
<p>Connecting a virtual machine to a network consists of two parts. First,
networks are specified in <code>spec.networks</code>. Then, interfaces backed by
the networks are added to the VM by specifying them in
<code>spec.domain.devices.interfaces</code>.</p>
<p>Each interface must have a corresponding network with the same name.</p>
<p>An <code>interface</code> defines a virtual network interface of a virtual machine
(also called a frontend). A <code>network</code> specifies the backend of an
<code>interface</code> and declares which logical or physical device it is
connected to (also called as backend).</p>
<p>There are multiple ways of configuring an <code>interface</code> as well as a
<code>network</code>.</p>
<p>All possible configuration options are available in the <a href="https://kubevirt.io/api-reference/master/definitions.html#_v1_interface">Interface API
Reference</a>
and <a href="https://kubevirt.io/api-reference/master/definitions.html#_v1_network">Network API
Reference</a>.</p>
<h2 id="backend">Backend<a class="headerlink" href="#backend" title="Permanent link">&para;</a></h2>
<p>Network backends are configured in <code>spec.networks</code>. A network must have
a unique name. Additional fields declare which logical or physical
device the network relates to.</p>
<p>Each network should declare its type by defining one of the following
fields:</p>
<table>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="header">
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><p><code>pod</code></p></td>
<td><p>Default Kubernetes network</p></td>
</tr>
<tr class="even">
<td><p><code>multus</code></p></td>
<td><p>Secondary network provided using Multus</p></td>
</tr>
</tbody>
</table>

<h3 id="pod">pod<a class="headerlink" href="#pod" title="Permanent link">&para;</a></h3>
<p>A <code>pod</code> network represents the default pod <code>eth0</code> interface configured
by cluster network solution that is present in each pod.</p>
<pre><code class="language-yaml">kind: VM
spec:
  domain:
    devices:
      interfaces:
        - name: default
          masquerade: {}
  networks:
  - name: default
    pod: {} # Stock pod network
</code></pre>
<h3 id="multus">multus<a class="headerlink" href="#multus" title="Permanent link">&para;</a></h3>
<p>It is also possible to connect VMIs to secondary networks using
<a href="https://github.com/intel/multus-cni">Multus</a>. This assumes that multus
is installed across your cluster and a corresponding
<code>NetworkAttachmentDefinition</code> CRD was created.</p>
<p>The following example defines a network which uses the <a href="https://github.com/kubevirt/ovs-cni">ovs-cni
plugin</a>, which will connect the VMI
to Open vSwitch's bridge <code>br1</code> and VLAN 100. Other CNI plugins such as
ptp, bridge, macvlan or Flannel might be used as well. For their
installation and usage refer to the respective project documentation.</p>
<p>First the <code>NetworkAttachmentDefinition</code> needs to be created. That is
usually done by an administrator. Users can then reference the
definition.</p>
<pre><code class="language-yaml">apiVersion: &quot;k8s.cni.cncf.io/v1&quot;
kind: NetworkAttachmentDefinition
metadata:
  name: ovs-vlan-100
spec:
  config: '{
      &quot;cniVersion&quot;: &quot;0.3.1&quot;,
      &quot;type&quot;: &quot;ovs&quot;,
      &quot;bridge&quot;: &quot;br1&quot;,
      &quot;vlan&quot;: 100
    }'
</code></pre>
<p>With following definition, the VMI will be connected to the default pod
network and to the secondary Open vSwitch network.</p>
<pre><code class="language-yaml">kind: VM
spec:
  domain:
    devices:
      interfaces:
        - name: default
          masquerade: {}
          bootOrder: 1   # attempt to boot from an external tftp server
          dhcpOptions:
            bootFileName: default_image.bin
            tftpServerName: tftp.example.com
        - name: ovs-net
          bridge: {}
          bootOrder: 2   # if first attempt failed, try to PXE-boot from this L2 networks
  networks:
  - name: default
    pod: {} # Stock pod network
  - name: ovs-net
    multus: # Secondary multus network
      networkName: ovs-vlan-100
</code></pre>
<p>It is also possible to define a multus network as the default pod
network with <a href="https://github.com/intel/multus-cni">Multus</a>. A version of
multus after this <a href="https://github.com/intel/multus-cni/pull/174">Pull
Request</a> is required
(currently master).</p>
<p><strong>Note the following:</strong></p>
<ul>
<li>
<p>A multus default network and a pod network type are mutually
    exclusive.</p>
</li>
<li>
<p>The virt-launcher pod that starts the VMI will <strong>not</strong> have the pod
    network configured.</p>
</li>
<li>
<p>The multus delegate chosen as default <strong>must</strong> return at least one
    IP address.</p>
</li>
</ul>
<p>Create a <code>NetworkAttachmentDefinition</code> with IPAM.</p>
<pre><code class="language-yaml">apiVersion: &quot;k8s.cni.cncf.io/v1&quot;
kind: NetworkAttachmentDefinition
metadata:
  name: macvlan-test
spec:
  config: '{
      &quot;type&quot;: &quot;macvlan&quot;,
      &quot;master&quot;: &quot;eth0&quot;,
      &quot;mode&quot;: &quot;bridge&quot;,
      &quot;ipam&quot;: {
        &quot;type&quot;: &quot;host-local&quot;,
        &quot;subnet&quot;: &quot;10.250.250.0/24&quot;
      }
    }'
</code></pre>
<p>Define a VMI with a <a href="https://github.com/intel/multus-cni">Multus</a>
network as the default.</p>
<pre><code class="language-yaml">kind: VM
spec:
  domain:
    devices:
      interfaces:
        - name: test1
          bridge: {}
  networks:
  - name: test1
    multus: # Multus network as default
      default: true
      networkName: macvlan-test
</code></pre>
<h4 id="invalid-cnis-for-secondary-networks">Invalid CNIs for secondary networks<a class="headerlink" href="#invalid-cnis-for-secondary-networks" title="Permanent link">&para;</a></h4>
<p>The following list of CNIs is known <strong>not</strong> to work for bridge interfaces -
which are most common for secondary interfaces.</p>
<ul>
<li>
<p><a href="https://www.cni.dev/plugins/current/main/macvlan/">macvlan</a></p>
</li>
<li>
<p><a href="https://www.cni.dev/plugins/current/main/ipvlan/">ipvlan</a></p>
</li>
</ul>
<p>The reason is similar: the bridge interface type moves the pod interface MAC
address to the VM, leaving the pod interface with a different address. The
aforementioned CNIs require the pod interface to have the original MAC address.</p>
<p>These issues are tracked individually:</p>
<ul>
<li>
<p><a href="https://github.com/kubevirt/kubevirt/issues/5483">macvlan</a></p>
</li>
<li>
<p><a href="https://github.com/kubevirt/kubevirt/issues/7001">ipvlan</a></p>
</li>
</ul>
<p>Feel free to discuss and / or propose fixes for them; we'd like to have
these plugins as valid options on our ecosystem.</p>
<h2 id="frontend">Frontend<a class="headerlink" href="#frontend" title="Permanent link">&para;</a></h2>
<p>Network interfaces are configured in <code>spec.domain.devices.interfaces</code>.
They describe properties of virtual interfaces as "seen" inside guest
instances. The same network backend may be connected to a virtual
machine in multiple different ways, each with their own connectivity
guarantees and characteristics.</p>
<p>Each interface should declare its type by defining on of the following
fields:</p>
<table>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="header">
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><p><code>bridge</code></p></td>
<td><p>Connect using a linux bridge</p></td>
</tr>
<tr class="even">
<td><p><code>slirp</code></p></td>
<td><p>Connect using QEMU user networking mode</p></td>
</tr>
<tr class="odd">
<td><p><code>sriov</code></p></td>
<td><p>Pass through a SR-IOV PCI device via <code>vfio</code></p></td>
</tr>
<tr class="even">
<td><p><code>masquerade</code></p></td>
<td><p>Connect using Iptables rules to nat the traffic</p></td>
</tr>
</tbody>
</table>

<p>Each interface may also have additional configuration fields that modify
properties "seen" inside guest instances, as listed below:</p>
<table>
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="header">
<th>Name</th>
<th>Format</th>
<th>Default value</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><p><code>model</code></p></td>
<td><p>One of: <code>e1000</code>, <code>e1000e</code>, <code>ne2k_pci</code>, <code>pcnet</code>, <code>rtl8139</code>, <code>virtio</code></p></td>
<td><p><code>virtio</code></p></td>
<td><p>NIC type</p></td>
</tr>
<tr class="even">
<td><p>macAddress</p></td>
<td><p><code>ff:ff:ff:ff:ff:ff</code> or <code>FF-FF-FF-FF-FF-FF</code></p></td>
<td></td>
<td><p>MAC address as seen inside the guest system, for example: <code>de:ad:00:00:be:af</code></p></td>
</tr>
<tr class="odd">
<td><p>ports</p></td>
<td></td>
<td><p>empty</p></td>
<td><p>List of ports to be forwarded to the virtual machine.</p></td>
</tr>
<tr class="even">
<td><p>pciAddress</p></td>
<td><p><code>0000:81:00.1</code></p></td>
<td></td>
<td><p>Set network interface PCI address, for example: <code>0000:81:00.1</code></p></td>
</tr>
</tbody>
</table>

<pre><code class="language-yaml">kind: VM
spec:
  domain:
    devices:
      interfaces:
        - name: default
          model: e1000 # expose e1000 NIC to the guest
          masquerade: {} # connect through a masquerade
          ports:
           - name: http
             port: 80
  networks:
  - name: default
    pod: {}
</code></pre>
<blockquote>
<p><strong>Note:</strong> For secondary interfaces, when a MAC address is specified for a
virtual machine interface, it is passed to the underlying CNI plugin which is,
in turn, expected to configure the backend to allow for this particular MAC.
Not every plugin has native support for custom MAC addresses.</p>
<p><strong>Note:</strong> For some CNI plugins without native support for custom MAC
addresses, there is a workaround, which is to use the <code>tuning</code> CNI
plugin to adjust pod interface MAC address. This can be used as
follows:</p>
</blockquote>
<pre><code class="language-yaml">apiVersion: &quot;k8s.cni.cncf.io/v1&quot;
kind: NetworkAttachmentDefinition
metadata:
  name: ptp-mac
spec:
  config: '{
      &quot;cniVersion&quot;: &quot;0.3.1&quot;,
      &quot;name&quot;: &quot;ptp-mac&quot;,
      &quot;plugins&quot;: [
        {
          &quot;type&quot;: &quot;ptp&quot;,
          &quot;ipam&quot;: {
            &quot;type&quot;: &quot;host-local&quot;,
            &quot;subnet&quot;: &quot;10.1.1.0/24&quot;
          }
        },
        {
          &quot;type&quot;: &quot;tuning&quot;
        }
      ]
    }'
</code></pre>
<blockquote>
<p>This approach may not work for all plugins. For example, OKD SDN is
not compatible with <code>tuning</code> plugin.</p>
<ul>
<li>
<p>Plugins that handle custom MAC addresses natively: <code>ovs</code>, <code>bridge</code>.</p>
</li>
<li>
<p>Plugins that are compatible with <code>tuning</code> plugin: <code>flannel</code>, <code>ptp</code>.</p>
</li>
<li>
<p>Plugins that don't need special MAC address treatment: <code>sriov</code> (in
    <code>vfio</code> mode).
</p>
</li>
</ul>
</blockquote>
<h3 id="ports">Ports<a class="headerlink" href="#ports" title="Permanent link">&para;</a></h3>
<p>Declare ports listen by the virtual machine</p>
<blockquote>
<p><strong>Note:</strong> When using the slirp interface only the configured ports
will be forwarded to the virtual machine.</p>
</blockquote>
<table>
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="header">
<th>Name</th>
<th>Format</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><p><code>name</code></p></td>
<td></td>
<td><p>no</p></td>
<td><p>Name</p></td>
</tr>
<tr class="even">
<td><p><code>port</code></p></td>
<td><p>1 - 65535</p></td>
<td><p>yes</p></td>
<td><p>Port to expose</p></td>
</tr>
<tr class="odd">
<td><p><code>protocol</code></p></td>
<td><p>TCP,UDP</p></td>
<td><p>no</p></td>
<td><p>Connection protocol</p></td>
</tr>
</tbody>
</table>

<blockquote>
<p><strong>Tip:</strong> Use <code>e1000</code> model if your guest image doesn't ship with
virtio drivers.</p>
<p><strong>Note:</strong> Windows machines need the latest virtio network driver to
configure the correct MTU on the interface.</p>
</blockquote>
<p>If <code>spec.domain.devices.interfaces</code> is omitted, the virtual machine is
connected using the default pod network interface of <code>bridge</code> type. If
you'd like to have a virtual machine instance without any network
connectivity, you can use the <code>autoattachPodInterface</code> field as follows:</p>
<pre><code class="language-yaml">kind: VM
spec:
  domain:
    devices:
      autoattachPodInterface: false
</code></pre>
<h3 id="bridge">bridge<a class="headerlink" href="#bridge" title="Permanent link">&para;</a></h3>
<p>In <code>bridge</code> mode, virtual machines are connected to the network backend
through a linux "bridge". The pod network IPv4 address is delegated to
the virtual machine via DHCPv4. The virtual machine should be configured
to use DHCP to acquire IPv4 addresses.</p>
<blockquote>
<p><strong>Note:</strong> If a specific MAC address is not configured in the virtual
machine interface spec the MAC address from the relevant pod interface
is delegated to the virtual machine.</p>
</blockquote>
<pre><code class="language-yaml">kind: VM
spec:
  domain:
    devices:
      interfaces:
        - name: red
          bridge: {} # connect through a bridge
  networks:
  - name: red
    multus:
      networkName: red
</code></pre>
<p>At this time, <code>bridge</code> mode doesn't support additional configuration
fields.</p>
<blockquote>
<p><strong>Note:</strong> due to IPv4 address delegation, in <code>bridge</code> mode the pod
doesn't have an IP address configured, which may introduce issues with
third-party solutions that may rely on it. For example, Istio may not
work in this mode.</p>
<p><strong>Note:</strong> admin can forbid using <code>bridge</code> interface type for pod
networks via a designated configuration flag. To achieve it, the admin
should set the following option to <code>false</code>:</p>
</blockquote>
<pre><code class="language-yaml">apiVersion: kubevirt.io/v1alpha3
kind: Kubevirt
metadata:
  name: kubevirt
  namespace: kubevirt
spec:
  configuration:
    network:
      permitBridgeInterfaceOnPodNetwork: false
</code></pre>
<blockquote>
<p><strong>Note:</strong> binding the pod network using <code>bridge</code> interface type may
cause issues. Other than the third-party issue mentioned in the above
note, live migration is not allowed with a pod network binding of
<code>bridge</code> interface type, and also some CNI plugins might not allow to
use a custom MAC address for your VM instances. If you think you may
be affected by any of issues mentioned above, consider changing the
default interface type to <code>masquerade</code>, and disabling the <code>bridge</code>
type for pod network, as shown in the example above.</p>
</blockquote>
<h3 id="slirp">slirp<a class="headerlink" href="#slirp" title="Permanent link">&para;</a></h3>
<p>In <code>slirp</code> mode, virtual machines are connected to the network backend
using QEMU user networking mode. In this mode, QEMU allocates internal
IP addresses to virtual machines and hides them behind NAT.</p>
<pre><code class="language-yaml">kind: VM
spec:
  domain:
    devices:
      interfaces:
        - name: red
          slirp: {} # connect using SLIRP mode
  networks:
  - name: red
    pod: {}
</code></pre>
<p>At this time, <code>slirp</code> mode doesn't support additional configuration
fields.</p>
<blockquote>
<p><strong>Note:</strong> in <code>slirp</code> mode, the only supported protocols are TCP and
UDP. ICMP is <em>not</em> supported.</p>
</blockquote>
<p>More information about SLIRP mode can be found in <a href="https://wiki.qemu.org/Documentation/Networking#User_Networking_.28SLIRP.29">QEMU
Wiki</a>.</p>
<h3 id="masquerade">masquerade<a class="headerlink" href="#masquerade" title="Permanent link">&para;</a></h3>
<p>In <code>masquerade</code> mode, KubeVirt allocates internal IP addresses to
virtual machines and hides them behind NAT. All the traffic exiting
virtual machines is "NAT'ed" using pod IP addresses. A guest operating system
should be configured to use DHCP to acquire IPv4 addresses.</p>
<p>To allow traffic of specific ports into virtual machines, the template <code>ports</code> section of
the interface should be configured as follows. If the <code>ports</code> section is missing,
all ports forwarded into the VM.</p>
<pre><code class="language-yaml">kind: VM
spec:
  domain:
    devices:
      interfaces:
        - name: red
          masquerade: {} # connect using masquerade mode
          ports:
            - port: 80 # allow incoming traffic on port 80 to get into the virtual machine
  networks:
  - name: red
    pod: {}
</code></pre>
<blockquote>
<p><strong>Note:</strong> Masquerade is only allowed to connect to the pod network.</p>
<p><strong>Note:</strong> The network CIDR can be configured in the pod network
section using the <code>vmNetworkCIDR</code> attribute.</p>
</blockquote>
<h4 id="masquerade-ipv4-and-ipv6-dual-stack-support">masquerade - IPv4 and IPv6 dual-stack support<a class="headerlink" href="#masquerade-ipv4-and-ipv6-dual-stack-support" title="Permanent link">&para;</a></h4>
<p><code>masquerade</code> mode can be used in IPv4 and IPv6 dual-stack clusters to provide
a VM with an IP connectivity over both protocols.</p>
<p>As with the IPv4 <code>masquerade</code> mode, the VM can be contacted using the pod's IP
address - which will be in this case two IP addresses, one IPv4 and one
IPv6. Outgoing traffic is also "NAT'ed" to the pod's respective IP address
from the given family.</p>
<p>Unlike in IPv4, the configuration of the IPv6 address and the default route is
not automatic; it should be configured via cloud init, as shown below:</p>
<pre><code class="language-yaml">kind: VM
spec:
  domain:
    devices:
      disks:
        - disk:
          bus: virtio
          name: cloudinitdisk
      interfaces:
        - name: red
          masquerade: {} # connect using masquerade mode
          ports:
            - port: 80 # allow incoming traffic on port 80 to get into the virtual machine
  networks:
  - name: red
    pod: {}
  volumes:
  - cloudInitNoCloud:
      networkData: |
        version: 2
        ethernets:
          eth0:
            dhcp4: true
            addresses: [ fd10:0:2::2/120 ]
            gateway6: fd10:0:2::1
      userData: |-
        #!/bin/bash
        echo &quot;fedora&quot; |passwd fedora --stdin
</code></pre>
<blockquote>
<p><strong>Note:</strong> The IPv6 address for the VM and default gateway <strong>must</strong> be the ones
shown above.</p>
</blockquote>
<h4 id="masquerade-ipv6-single-stack-support">masquerade - IPv6 single-stack support<a class="headerlink" href="#masquerade-ipv6-single-stack-support" title="Permanent link">&para;</a></h4>
<p><code>masquerade</code> mode can be used in IPv6 single stack clusters to provide a VM
with an IPv6 only connectivity.</p>
<p>As with the IPv4 <code>masquerade</code> mode, the VM can be contacted using the pod's IP
address - which will be in this case the IPv6 one.
Outgoing traffic is also "NAT'ed" to the pod's respective IPv6 address.</p>
<p>As with the dual-stack cluster, the configuration of the IPv6 address and the default route is
not automatic; it should be configured via cloud init, as shown in the <a href="#masquerade-ipv4-and-ipv6-dual-stack-support">dual-stack section</a>.</p>
<p>Unlike the dual-stack cluster, which has a DHCP server for IPv4, the IPv6 single stack cluster
has no DHCP server at all. Therefore, the VM won't have the search domains information and
reaching a destination using its FQDN is not possible.
Tracking issue - https://github.com/kubevirt/kubevirt/issues/7184</p>
<h3 id="passt">passt<a class="headerlink" href="#passt" title="Permanent link">&para;</a></h3>
<p><code>passt</code> is a new approach for user-mode networking which can be used as a simple replacement for Slirp (which is practically dead).</p>
<p><code>passt</code> is a universal tool which implements a translation layer between a Layer-2 network interface and native
Layer -4 sockets (TCP, UDP, ICMP/ICMPv6 echo) on a host.<br/></p>
<p>Its main benefits are:
- doesn't require extra network capabilities as CAP_NET_RAW and CAP_NET_ADMIN.
- allows integration with service meshes (which expect applications to run locally) out of the box.
- supports IPv6 out of the box (in contrast to the existing bindings which require configuring IPv6
manually).</p>
<table>
<thead>
<tr>
<th></th>
<th>Masquerade</th>
<th>Bridge</th>
<th>Passt</th>
</tr>
</thead>
<tbody>
<tr>
<td>Supports migration</td>
<td>Yes</td>
<td>No</td>
<td>No<br/>(will be supported in the future)</td>
</tr>
<tr>
<td>VM uses Pod IP</td>
<td>No</td>
<td>Yes</td>
<td>Yes<br/>(in the future it will be possible to configure the VM IP. Currently the default is the pod IP)</td>
</tr>
<tr>
<td>Service Mesh out of the box</td>
<td>No<br/>(only ISTIO is supported, adjustmets on both ISTIO and kubevirt had to be done to make it work)</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr>
<td>Doesn’t require extra capabilities on the virt-launcher pod</td>
<td>Yes<br/>(multiple workarounds had to be added to kuebivrt to make it work)</td>
<td>No<br/>(Multiple workarounds had to be added to kuebivrt to make it work)</td>
<td>Yes</td>
</tr>
<tr>
<td>Doesn't require extra network devices on the virt-launcher pod</td>
<td>No<br/>(bridge and tap device are created)</td>
<td>No<br/>(bridge and tap device are created)</td>
<td>Yes</td>
</tr>
<tr>
<td>Supports IPv6</td>
<td>Yes<br/>(requires manual configuration on the VM)</td>
<td>No</td>
<td>Yes</td>
</tr>
</tbody>
</table>
<pre><code class="language-yaml">kind: VM
spec:
  domain:
    devices:
      interfaces:
        - name: red
          passt: {} # connect using passt mode
          ports:
            - port: 8080 # allow incoming traffic on port 8080 to get into the virtual machine
  networks:
    - name: red
      pod: {}
</code></pre>
<h4 id="requirementsrecommendations">Requirements/Recommendations:<a class="headerlink" href="#requirementsrecommendations" title="Permanent link">&para;</a></h4>
<ol>
<li>To get better performance the node should be configured with:</li>
</ol>
<pre><code>sysctl -w net.core.rmem_max = 33554432
sysctl -w net.core.wmem_max = 33554432
</code></pre>
<ol>
<li>To run multiple passt VMs with no explicit ports, the node's <code>fs.file-max</code> should be increased
   (for a VM forwards all IPv4 and IPv6 ports, for TCP and UDP, passt needs to create ~2^18 sockets):</li>
</ol>
<pre><code>sysctl -w fs.file-max = 9223372036854775807
</code></pre>
<ol>
<li>The <code>vmi.Spec.Domain.Resources.Requests.Memory</code> of a passt VM with no explicit ports should be at least 2048M.</li>
</ol>
<h4 id="temporary-restrictions">Temporary restrictions:<a class="headerlink" href="#temporary-restrictions" title="Permanent link">&para;</a></h4>
<ol>
<li><code>passt</code> currently only supported as primary network and doesn't allow extra multus networks to be configured on the VM.</li>
<li>A VM with passt binding cannot bind to low ports (less than 1024).</li>
</ol>
<p>passt interfaces are feature gated; to enable the feature, follow
<a href="../../operations/activating_feature_gates/#how-to-activate-a-feature-gate">these</a>
instructions, in order to activate the <code>Passt</code> feature gate (case sensitive).</p>
<p>More information about passt mode can be found in <a href="https://passt.top/passt/about/">passt
Wiki</a>.</p>
<h3 id="virtio-net-multiqueue">virtio-net multiqueue<a class="headerlink" href="#virtio-net-multiqueue" title="Permanent link">&para;</a></h3>
<p>Setting the <code>networkInterfaceMultiqueue</code> to <code>true</code> will enable the
multi-queue functionality, increasing the number of vhost queue, for
interfaces configured with a <code>virtio</code> model.</p>
<pre><code class="language-yaml">kind: VM
spec:
  domain:
    devices:
      networkInterfaceMultiqueue: true
</code></pre>
<p>Users of a Virtual Machine with multiple vCPUs may benefit of increased
network throughput and performance.</p>
<p>Currently, the number of queues is being determined by the number of
vCPUs of a VM. This is because multi-queue support optimizes RX
interrupt affinity and TX queue selection in order to make a specific
queue private to a specific vCPU.</p>
<p>Without enabling the feature, network performance does not scale as the
number of vCPUs increases. Guests cannot transmit or retrieve packets in
parallel, as virtio-net has only one TX and RX queue.</p>
<p>Virtio interfaces advertise on their status.interfaces.interface entry a field named queueCount.<br />
The queueCount field indicates how many queues were assigned to the interface.<br />
Queue count value is derived from the domain XML.<br />
In case the number of queues can't be determined (i.e interface that is reported by quest-agent only),
it will be omitted.</p>
<p><em>NOTE</em>: Although the virtio-net multiqueue feature provides a
performance benefit, it has some limitations and therefore should not be
unconditionally enabled</p>
<h4 id="some-known-limitations">Some known limitations<a class="headerlink" href="#some-known-limitations" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p>Guest OS is limited to ~200 MSI vectors. Each NIC queue requires a
    MSI vector, as well as any virtio device or assigned PCI device.
    Defining an instance with multiple virtio NICs and vCPUs might lead
    to a possibility of hitting the guest MSI limit.</p>
</li>
<li>
<p>virtio-net multiqueue works well for incoming traffic, but can
    occasionally cause a performance degradation, for outgoing traffic.
    Specifically, this may occur when sending packets under 1,500 bytes
    over the Transmission Control Protocol (TCP) stream.</p>
</li>
<li>
<p>Enabling virtio-net multiqueue increases the total network
    throughput, but in parallel it also increases the CPU consumption.</p>
</li>
<li>
<p>Enabling virtio-net multiqueue in the host QEMU config, does not
    enable the functionality in the guest OS. The guest OS administrator
    needs to manually turn it on for each guest NIC that requires this
    feature, using ethtool.</p>
</li>
<li>
<p>MSI vectors would still be consumed (wasted), if multiqueue was
    enabled in the host, but has not been enabled in the guest OS by the
    administrator.</p>
</li>
<li>
<p>In case the number of vNICs in a guest instance is proportional to
    the number of vCPUs, enabling the multiqueue feature is less
    important.</p>
</li>
<li>
<p>Each virtio-net queue consumes 64 KiB of kernel memory for the vhost
    driver.</p>
</li>
</ul>
<p><em>NOTE</em>: Virtio-net multiqueue should be enabled in the guest OS
manually, using ethtool. For example:
<code>ethtool -L &lt;NIC&gt; combined #num_of_queues</code></p>
<p>More information please refer to <a href="http://www.linux-kvm.org/page/Multiqueue">KVM/QEMU
MultiQueue</a>.</p>
<h3 id="sriov">sriov<a class="headerlink" href="#sriov" title="Permanent link">&para;</a></h3>
<p>In <code>sriov</code> mode, virtual machines are directly exposed to an SR-IOV PCI
device, usually allocated by <a href="https://github.com/intel/sriov-network-device-plugin">Intel SR-IOV device
plugin</a>. The
device is passed through into the guest operating system as a host
device, using the
<a href="https://www.kernel.org/doc/Documentation/vfio.txt">vfio</a> userspace
interface, to maintain high networking performance.</p>
<h4 id="how-to-expose-sr-iov-vfs-to-kubevirt">How to expose SR-IOV VFs to KubeVirt<a class="headerlink" href="#how-to-expose-sr-iov-vfs-to-kubevirt" title="Permanent link">&para;</a></h4>
<p>To simplify procedure, please use <a href="https://github.com/openshift/sriov-network-operator">OpenShift SR-IOV
operator</a> to deploy
and configure SR-IOV components in your cluster. On how to use the
operator, please refer to <a href="https://github.com/openshift/sriov-network-operator/blob/master/doc/quickstart.md">their respective
documentation</a>.</p>
<blockquote>
<p><strong>Note:</strong> KubeVirt relies on VFIO userspace driver to pass PCI devices
into VMI guest. Because of that, when configuring SR-IOV operator
policies, make sure you define a pool of VF resources that uses
<code>driver: vfio</code>.</p>
</blockquote>
<p>Once the operator is deployed, an <a href="https://github.com/openshift/sriov-network-operator#sriovnetworknodeconfigpolicy">SriovNetworkNodePolicy
</a>
must be provisioned, in which the list of SR-IOV devices to expose (with
respective configurations) is defined.</p>
<p>Please refer to the following <code>SriovNetworkNodePolicy</code> for an example:</p>
<pre><code class="language-yaml">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: policy-1
  namespace: sriov-network-operator
spec:
  deviceType: vfio-pci
  mtu: 9000
  nicSelector:
    pfNames:
    - ens1f0
  nodeSelector:
    sriov: &quot;true&quot;
  numVfs: 8
  priority: 90
  resourceName: sriov-nic
</code></pre>
<p>The policy above will configure the <code>SR-IOV</code> device plugin, allowing the
PF named <code>ens1f0</code> to be exposed in the SRIOV capable nodes as a resource named
<code>sriov-nic</code>.</p>
<h4 id="start-an-sr-iov-vm">Start an SR-IOV VM<a class="headerlink" href="#start-an-sr-iov-vm" title="Permanent link">&para;</a></h4>
<p>Once all the SR-IOV components are deployed, it is needed to indicate how to
configure the SR-IOV network. Refer to the following
<code>SriovNetwork</code> for an example:</p>
<pre><code class="language-yaml">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
  name: sriov-net
  namespace: sriov-network-operator
spec:
  ipam: |
    {}
  networkNamespace: default
  resourceName: sriov-nic
  spoofChk: &quot;off&quot;
</code></pre>
<p>Finally, to create a VM that will attach to the aforementioned Network, refer
to the following VMI spec:</p>
<pre><code class="language-yaml">---
apiVersion: kubevirt.io/v1alpha3
kind: VirtualMachineInstance
metadata:
  labels:
    special: vmi-perf
  name: vmi-perf
spec:
  domain:
    cpu:
      sockets: 2
      cores: 1
      threads: 1
      dedicatedCpuPlacement: true
    resources:
      requests:
        memory: &quot;4Gi&quot;
      limits:
        memory: &quot;4Gi&quot;
    devices:
      disks:
      - disk:
          bus: virtio
        name: containerdisk
      - disk:
          bus: virtio
        name: cloudinitdisk
      interfaces:
      - masquerade: {}
        name: default
      - name: sriov-net
        sriov: {}
      rng: {}
    machine:
      type: &quot;&quot;
  networks:
  - name: default
    pod: {}
  - multus:
      networkName: default/sriov-net
    name: sriov-net
  terminationGracePeriodSeconds: 0
  volumes:
  - containerDisk:
      image: docker.io/kubevirt/fedora-cloud-container-disk-demo:latest
    name: containerdisk
  - cloudInitNoCloud:
      userData: |
        #!/bin/bash
        echo &quot;centos&quot; |passwd centos --stdin
        dhclient eth1
    name: cloudinitdisk
</code></pre>
<blockquote>
<p><strong>Note:</strong> for some NICs (e.g. Mellanox), the kernel module needs to be
installed in the guest VM.</p>
<p><strong>Note:</strong> Placement on dedicated CPUs can only be achieved if the Kubernetes CPU manager is running on the SR-IOV capable workers.
For further details please refer to the <a href="https://kubevirt.io/user-guide/#/creation/dedicated-cpu">dedicated cpu resources documentation</a>.</p>
</blockquote>
<h3 id="macvtap">Macvtap<a class="headerlink" href="#macvtap" title="Permanent link">&para;</a></h3>
<p>In <code>macvtap</code> mode, virtual machines are directly exposed to the Kubernetes
nodes L2 network. This is achieved by 'extending' an existing network interface
with a virtual device that has its own MAC address.</p>
<p>Macvtap interfaces are feature gated; to enable the feature, follow
<a href="../../operations/activating_feature_gates/#how-to-activate-a-feature-gate">these</a>
instructions, in order to activate the <code>Macvtap</code> feature gate (case sensitive).</p>
<h4 id="limitations">Limitations<a class="headerlink" href="#limitations" title="Permanent link">&para;</a></h4>
<ul>
<li>Live migration is not seamless, see <a href="https://github.com/kubevirt/kubevirt/issues/5912#issuecomment-888938920">issue #5912</a></li>
</ul>
<h4 id="how-to-expose-host-interface-to-the-macvtap-device-plugin">How to expose host interface to the macvtap device plugin<a class="headerlink" href="#how-to-expose-host-interface-to-the-macvtap-device-plugin" title="Permanent link">&para;</a></h4>
<p>To simplify the procedure, please use the
<a href="https://github.com/kubevirt/cluster-network-addons-operator">Cluster Network Addons Operator</a>
to deploy and configure the macvtap components in your cluster.</p>
<p>The aforementioned operator effectively deploys the
<a href="https://github.com/kubevirt/macvtap-cni">macvtap-cni</a> cni / device plugin
combo.</p>
<p>There are two different alternatives to configure which host interfaces get
exposed to the user, enabling them to create macvtap interfaces on top of:</p>
<ul>
<li>select the host interfaces: indicates which host interfaces are exposed.</li>
<li>expose all interfaces: all interfaces of all hosts are exposed.</li>
</ul>
<p>Both options are configured via the <code>macvtap-deviceplugin-config</code> ConfigMap,
and more information on how to configure it can be found in the
<a href="https://github.com/kubevirt/macvtap-cni#deployment">macvtap-cni</a> repo.</p>
<p>You can find a minimal example, in which the <code>eth0</code> interface of the Kubernetes
nodes is exposed, via the <code>master</code> attribute.</p>
<pre><code class="language-yaml">kind: ConfigMap
apiVersion: v1
metadata:
  name: macvtap-deviceplugin-config
data:
  DP_MACVTAP_CONF: |
    [
        {
            &quot;name&quot;     : &quot;dataplane&quot;,
            &quot;master&quot;   : &quot;eth0&quot;,
            &quot;mode&quot;     : &quot;bridge&quot;,
            &quot;capacity&quot; : 50
        },
    ]
</code></pre>
<p>This step can be omitted, since the default configuration of the aforementioned
<code>ConfigMap</code> is to expose all host interfaces (which is represented by the
following configuration):</p>
<pre><code class="language-yaml">kind: ConfigMap
apiVersion: v1
metadata:
  name: macvtap-deviceplugin-config
data:
  DP_MACVTAP_CONF: '[]'
</code></pre>
<h4 id="start-a-vm-with-macvtap-interfaces">Start a VM with macvtap interfaces<a class="headerlink" href="#start-a-vm-with-macvtap-interfaces" title="Permanent link">&para;</a></h4>
<p>Once the macvtap components are deployed, it is needed to indicate how to
configure the macvtap network. Refer to the following
<code>NetworkAttachmentDefinition</code> for a simple example:</p>
<pre><code class="language-yaml">---
kind: NetworkAttachmentDefinition
apiVersion: k8s.cni.cncf.io/v1
metadata:
  name: macvtapnetwork
  annotations:
    k8s.v1.cni.cncf.io/resourceName: macvtap.network.kubevirt.io/eth0
spec:
  config: '{
      &quot;cniVersion&quot;: &quot;0.3.1&quot;,
      &quot;name&quot;: &quot;macvtapnetwork&quot;,
      &quot;type&quot;: &quot;macvtap&quot;,
      &quot;mtu&quot;: 1500
    }'
</code></pre>
<p>The requested <code>k8s.v1.cni.cncf.io/resourceName</code> annotation must point to an
exposed host interface (via the <code>master</code> attribute, on the
<code>macvtap-deviceplugin-config</code> <code>ConfigMap</code>).</p>
<p>Finally, to create a VM that will attach to the aforementioned Network, refer
to the following VMI spec:</p>
<pre><code class="language-yaml">---
apiVersion: kubevirt.io/v1alpha3
kind: VirtualMachineInstance
metadata:
  labels:
    special: vmi-host-network
  name: vmi-host-network
spec:
  domain:
    devices:
      disks:
      - disk:
          bus: virtio
        name: containerdisk
      - disk:
          bus: virtio
        name: cloudinitdisk
      interfaces:
      - macvtap: {}
        name: hostnetwork
      rng: {}
    machine:
      type: &quot;&quot;
    resources:
      requests:
        memory: 1024M
  networks:
  - multus:
      networkName: macvtapnetwork
    name: hostnetwork
  terminationGracePeriodSeconds: 0
  volumes:
  - containerDisk:
      image: docker.io/kubevirt/fedora-cloud-container-disk-demo:devel
    name: containerdisk
  - cloudInitNoCloud:
      userData: |-
        #!/bin/bash
        echo &quot;fedora&quot; |passwd fedora --stdin
    name: cloudinitdisk
</code></pre>
<p>The requested <code>multus</code> <code>networkName</code> - i.e. <code>macvtapnetwork</code> - must match the
name of the provisioned <code>NetworkAttachmentDefinition</code>.</p>
<blockquote>
<p><strong>Note:</strong> VMIs with macvtap interfaces can be migrated, but their MAC
addresses <strong>must</strong> be statically set.</p>
</blockquote>
<h2 id="security">Security<a class="headerlink" href="#security" title="Permanent link">&para;</a></h2>
<h3 id="mac-spoof-check">MAC spoof check<a class="headerlink" href="#mac-spoof-check" title="Permanent link">&para;</a></h3>
<p>MAC spoofing refers to the ability to generate traffic with an arbitrary source
MAC address.
An attacker may use this option to generate attacks on the network.</p>
<p>In order to protect against such scenarios, it is possible to enable the
mac-spoof-check support in CNI plugins that support it.</p>
<p>The pod primary network which is served by the cluster network provider
is not covered by this documentation. Please refer to the relevant provider to
check how to enable spoofing check.
The following text refers to the secondary networks, served using multus.</p>
<p>There are two known CNI plugins that support mac-spoof-check:</p>
<ul>
<li><a href="https://github.com/openshift/sriov-cni">sriov-cni</a>:
  Through the <code>spoofchk</code> parameter .</li>
<li>cnv-bridge: Through the <code>macspoofchk</code>.</li>
</ul>
<blockquote>
<p><strong>Note:</strong> <code>cnv-bridge</code> is provided by
  <a href="https://github.com/kubevirt/cluster-network-addons-operator">CNAO</a>.
  The <a href="https://github.com/containernetworking/plugins">bridge-cni</a> is planned
  to support the <code>macspoofchk</code> options as well.</p>
</blockquote>
<p>The configuration is to be done on the  NetworkAttachmentDefinition by the
operator and any interface that refers to it, will have this feature enabled.</p>
<p>Below is an example of using the <code>cnv-bridge</code> CNI with <code>macspoofchk</code> enabled:</p>
<pre><code class="language-yaml">apiVersion: &quot;k8s.cni.cncf.io/v1&quot;
kind: NetworkAttachmentDefinition
metadata:
  name: br-spoof-check
spec:
  config: '{
            &quot;cniVersion&quot;: &quot;0.3.1&quot;,
            &quot;name&quot;: &quot;br-spoof-check&quot;,
            &quot;type&quot;: &quot;cnv-bridge&quot;,
            &quot;bridge&quot;: &quot;br10&quot;,
            &quot;macspoofchk&quot;: true
        }'
</code></pre>
<p>On the VMI, the network section should point to this
NetworkAttachmentDefinition by name:</p>
<pre><code class="language-yaml">  networks:
  - name: default
    pod: {}
  - multus:
      networkName: br-spoof-check
    name: br10
</code></pre>
<h4 id="limitations_1">Limitations<a class="headerlink" href="#limitations_1" title="Permanent link">&para;</a></h4>
<ul>
<li>The <code>cnv-bridge</code> CNI supports mac-spoof-check through nftables, therefore
the node must support nftables and have the <code>nft</code> binary deployed.</li>
</ul>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../disks_and_volumes/" class="btn btn-neutral float-left" title="Disks and Volumes"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../istio_service_mesh/" class="btn btn-neutral float-right" title="Istio service mesh">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
        <span>
          <a href="https://github.com/kubevirt/user-guide/" class="fa fa-github" style="color: #fcfcfc"> GitHub</a>
        </span>
    
    
      <span><a href="../disks_and_volumes/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../istio_service_mesh/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script>var base_url = '../..';</script>
    <script src="../../js/theme_extra.js" defer></script>
    <script src="../../js/theme.js" defer></script>
      <script src="../../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
